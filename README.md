# ğŸ™ï¸ The Empathy Engine  
### Giving AI a Human Voice â€” with Emotional Resonance

---

## ğŸŒ Overview

**The Empathy Engine** is a Python-based project that converts plain text into emotionally expressive speech.  
It detects the underlying **emotion** of input text, dynamically adjusts **vocal parameters** such as rate, pitch, and volume, and produces speech that feels *authentic, empathetic,* and *contextually aware.*

Traditional Text-to-Speech (TTS) systems sound robotic because they apply uniform tone across an entire sentence. The Empathy Engine solves this by performing **phrase-level emotion mapping**, ensuring that only emotionally charged words or sections are expressed â€” just as humans naturally do.

Even if the overall emotion classification is slightly wrong, the system still sounds human because it modulates speech subtly, preserving clarity and natural rhythm.

---

## ğŸ“ Project Structure
```
empathy-engine/
â”œâ”€â”€ app.py                    # Main Flask application (250 lines)
â”œâ”€â”€ cli.py                    # Command-line interface (180 lines)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 
â”œâ”€â”€ setup.sh / setup.bat      # Automated setup scripts
â”œâ”€â”€ .gitignore                # Git ignore rules
â””â”€â”€ templates/
    â””â”€â”€ index.html            # Web UI (350+ lines)
```

---

## âš™ï¸ Quick Setup (Windows)

### 1ï¸âƒ£ Run the Automated Setup Script

Simply double-click or run the setup batch file:
```bash
setup.bat
```

This script will:
- Verify Python installation
- Create a virtual environment
- Install all dependencies from requirements.txt
- Create necessary folders (templates, audio_output)

Once setup completes:
```bash
python app.py
```

Then open your browser at:
```
http://localhost:5000
```

âš ï¸ Important: Use http:// (not https://)

---

## âš™ï¸ Quick Setup (Linux/Mac)

Use the provided setup.sh:
```bash
chmod +x setup.sh
./setup.sh
```

Then run:
```bash
python app.py
```

Open:
```
http://localhost:5000
```

---

## ğŸ“¦ Requirements

Dependencies are listed in requirements.txt:
```
flask==3.0.0
transformers==4.36.0
torch==2.5.1
pyttsx3==2.90
accelerate==0.25.0
nltk==3.9.1
textblob==0.18.0
pydub==0.25.1
```

ğŸ’¡ Optional: Install ffmpeg for smoother audio concatenation (used by pydub).

---

## ğŸ§  How It Works

### Step 1 â€” Emotion Detection

The Empathy Engine uses a hybrid ensemble:

- **Transformer Model (Hugging Face)**: Contextual emotion detection.
- **Lexicon-based Analysis (NLTK/TextBlob)**: Keyword-based reinforcement.
- **Pattern-based Heuristics**: Detects emotional punctuation, emojis, or emphasis.

The ensemble computes a combined emotional score for each emotion:
```
final_score[e] = 0.5 * transformer + 0.25 * lexicon + 0.25 * pattern
```

The emotion with the highest weighted score becomes the dominant emotion.

### Step 2 â€” Intensity Scaling

Emotion intensity determines how strongly we modulate the voice.
A subtle or uncertain emotion leads to gentler modulation, while a strong emotion increases expressiveness.
```
intensity = 0.5 + (confidence * 1.0)
```

| Confidence | Intensity | Effect              |
|------------|-----------|---------------------|
| 0.3        | 0.8       | Soft modulation     |
| 0.6        | 1.1       | Moderate emotion    |
| 1.0        | 1.5       | Full emotional tone |

### Step 3 â€” Emotion â†’ Voice Mapping

| Emotion  | Rate (WPM) | Pitch | Volume |
|----------|------------|-------|--------|
| Joy      | 200        | 180   | 0.95   |
| Sadness  | 130        | 80    | 0.70   |
| Anger    | 190        | 160   | 1.00   |
| Fear     | 210        | 190   | 0.85   |
| Surprise | 195        | 200   | 0.90   |
| Disgust  | 140        | 90    | 0.75   |
| Neutral  | 165        | 120   | 0.85   |

The final output is a smooth interpolation between neutral and emotion-specific parameters:
```
final_param = neutral_param + (emotion_param - neutral_param) * intensity
```

This formula ensures natural speech dynamics and prevents unnatural exaggeration.

### Step 4 â€” Phrase-Level Modulation

Human prosody isn't constant. People emphasize emotional phrases, not every word.
The Empathy Engine follows the same principle:

âœ… Emotional keywords are emphasized (faster, louder, or higher-pitched).  
âœ… Neutral parts remain steady and intelligible.  
âœ… Emphasis patterns come from detected emotional spans using lexicon and punctuation.

This results in expressive but controlled delivery â€” emotional but never overacted.

### Step 5 â€” Graceful Misclassification Handling

Even if the detected overall emotion is wrong:

- Phrase-level emotional cues still sound correct.
- Neutral text portions stay unaltered.
- Low confidence reduces intensity automatically.

This mirrors natural human variation â€” people don't speak entirely in one tone even when uncertain about their feelings.

ğŸ§ The result: emotionally expressive yet balanced audio output, even in imperfect cases.

---

## ğŸ§® Scoring Logic Justification

| Component          | Description                                                      | Weight |
|--------------------|------------------------------------------------------------------|--------|
| Emotion Detection  | Ensemble detection combining semantic + lexical + heuristic     | 40%    |
| Voice Modulation   | Rate, pitch, and volume dynamically adapted                      | 25%    |
| Naturalness        | Phrase-level control, avoiding overexpression                    | 20%    |
| Error Handling     | Smooth degradation when emotion misclassified                    | 10%    |
| User Interface     | Flask-based web and CLI interfaces                               | 5%     |

**Philosophy:**  
Speech should feel human, not just sound human.  
The engine scores high on usability, emotion clarity, and consistency even with noisy input.

---

## ğŸ§° Components

### 1ï¸âƒ£ app.py
Core Flask web service

**Routes:**
- `/` â†’ Web UI
- `/analyze` â†’ POST endpoint for emotion detection + audio generation
- `/audio/<file>` â†’ Serve generated .wav files

Handles emotion-to-voice mapping, intensity scaling, and phrase-level synthesis.

### 2ï¸âƒ£ cli.py
Lightweight command-line version

- Accepts text input directly from terminal
- Outputs emotion classification + generated audio file
- Perfect for testing without the web UI

### 3ï¸âƒ£ templates/index.html
Intuitive browser-based interface

- Allows real-time text input and playback
- Displays detected emotion and generated audio

---

## ğŸ”Š Output Example

**Input:**
```
"I just got the best news ever!"
```

**Output:**
```json
{
  "success": true,
  "emotion": "joy",
  "confidence": 0.82,
  "intensity": 1.32,
  "voice_parameters": {"rate": 210, "pitch": 180, "volume": 0.95},
  "audio_file": "speech_joy_20251107.wav"
}
```

**Play via:**
```
http://localhost:5000/audio/speech_joy_20251107.wav
```

---

## ğŸ§© CLI Usage Example

Run from the command line:
```bash
python cli.py "I'm so excited for tomorrow's event!"
```

**Example output:**
```
Detected Emotion: joy (confidence: 0.81)
Generated File: output_speech_joy.wav
Voice Parameters: rate=210, pitch=180, volume=0.95
```

---

## ğŸ§  Design Philosophy

**"Empathy isn't in the words we say â€” it's in how we say them."**

The Empathy Engine was built to bring this human nuance to AI-generated speech.  
It bridges sentiment understanding and emotional vocal delivery through dynamic modulation, creating trust and warmth in every spoken phrase.

---

## ğŸš€ Future Enhancements

ğŸ”¸ **SSML Support**: Add markup-based emphasis, pauses, and phonetic tuning.  
ğŸ”¸ **Neural TTS Integration**: Replace pyttsx3 with expressive cloud voices (e.g., Google, ElevenLabs).  
ğŸ”¸ **Multilingual Emotion Detection**: Add multilingual transformer support.  
ğŸ”¸ **Contextual Memory**: Carry emotional state across multiple sentences.  
ğŸ”¸ **Live Speech Interaction**: Real-time voice response through microphone input.

---

## ğŸ‘¥ Credits

Developed with â¤ï¸ for **Challenge 1: The Empathy Engine â€” Giving AI a Human Voice**

Built using:
- Python 3.8+
- Flask
- Hugging Face Transformers
- NLTK / TextBlob
- pyttsx3 / pydub

---

## ğŸ–¥ï¸ Final Run Summary
```bash
setup.bat
python app.py
# Then open:
http://localhost:5000
```

or use the CLI:
```bash
python cli.py "Your text here"
```

---

ğŸ’¬ **"The goal isn't just to make AI speak â€” it's to make it care how it sounds."**
